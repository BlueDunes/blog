<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>DataMary</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2022-09-20T12:55:25-07:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>PCA step by step with R</title>
   <link href="http://localhost:4000/2022/09/16/PCA_R/"/>
   <updated>2022-09-16T00:00:00-07:00</updated>
   <id>http://localhost:4000/2022/09/16/PCA_R</id>
   <content type="html">&lt;p&gt;If we are working with high dimensional data we will most probably want to reduce the dimensionality before we start working on our models. One option we have is to drop features that we consider less relevant, either because they contain a high proportion of missing values, because they have low variance or because their correlation with the target variable is low. Another option, or sometimes a second step after dropping a few columns, is using some kind of regularization technique. Principal Components Analysis (PCA) is a very commonly used one that is based in linear algebra concepts. &lt;!--more--&gt;On top of reducing dimensionality, it also allows us to better represent the information contained in our data by determining which variables are more relevant.&lt;/p&gt;

&lt;p&gt;We will work with the popular Boston Housing dataset available on the UCI Machine Learning Repository &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The original creators of the dataset are: Harrison, D. and Rubinfeld, D.L.&lt;/li&gt;
  &lt;li&gt;The dataset has no missing values.&lt;/li&gt;
  &lt;li&gt;Number of Instances: 506&lt;/li&gt;
  &lt;li&gt;Number of Attributes: 13 continuous attributes (including “class” attribute “MEDV”) and 1 binary-valued attribute.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are the variables:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;CRIM&lt;/strong&gt;: per capita crime rate by town&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ZN&lt;/strong&gt;: proportion of residential land zoned for lots over 25,000 sq.ft.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;INDUS&lt;/strong&gt;: proportion of non-retail business acres per town&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CHAS&lt;/strong&gt;: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;NOX&lt;/strong&gt;: nitric oxides concentration (parts per 10 million)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RM&lt;/strong&gt;: average number of rooms per dwelling&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AGE:&lt;/strong&gt; proportion of owner-occupied units built prior to 1940&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DIS:&lt;/strong&gt; weighted distances to five Boston employment centres&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RAD&lt;/strong&gt;: index of accessibility to radial highways&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TAX&lt;/strong&gt;: full-value property-tax rate per $10,000&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PTRATIO&lt;/strong&gt;: pupil-teacher ratio by town&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;B&lt;/strong&gt;: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LSTAT&lt;/strong&gt;: % lower status of the population&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MEDV&lt;/strong&gt;: Median value of owner-occupied homes in $1000’s&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this post we will:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Observe the correlation of the different independent variables with our target variable.&lt;/li&gt;
  &lt;li&gt;Perform a principal components analysis PCA. We will use both dedicated R funcions and the step by step process so that we can better understand what goes on behind the scenes.&lt;/li&gt;
  &lt;li&gt;Build a quick regression model that can be used to predict housing prices based on the features available. Therefore, “MEDV” will be our target variable. This is just a taste of what one could do after PCA.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;preparation-of-the-vectors&quot;&gt;Preparation of the vectors&lt;/h2&gt;
&lt;p&gt;Here we can see the structure of our dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;str(housing)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
'data.frame':	506 obs. of  14 variables:
 $ CRIM   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
 $ ZN     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
 $ INDUS  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
 $ CHAS   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ NOX    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
 $ RM     : num  6.58 6.42 7.18 7 7.15 ...
 $ AGE    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
 $ DIS    : num  4.09 4.97 4.97 6.06 6.06 ...
 $ RAD    : int  1 2 2 3 3 3 5 5 5 5 ...
 $ TAX    : num  296 242 242 222 222 222 311 311 311 311 ...
 $ PTRATIO: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
 $ B      : num  397 397 393 395 397 ...
 $ LSTAT  : num  4.98 9.14 4.03 2.94 5.33 ...
 $ MEDV   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First we generate a price vector containing the variable “MEDV”.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;price &amp;lt;- housing$MEDV
head(price)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
[1] 24.0 21.6 34.7 33.4 36.2 28.7&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We check the type of data that it contains, which is numeric in this case.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class(price)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
[1] &quot;numeric&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We create a matrix “features” with 506 rows and 13 columns that contains our independent variables.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;features &amp;lt;- as.matrix(housing[,1:13])
head(features)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
        CRIM ZN INDUS CHAS   NOX    RM  AGE    DIS RAD TAX PTRATIO      B LSTAT
[1,] 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
[2,] 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
[3,] 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
[4,] 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
[5,] 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
[6,] 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We check that we are indeed working with a matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class(features)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
[1] &quot;matrix&quot; &quot;array&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The function &lt;code&gt;summary&lt;/code&gt; allows us to have a general view of the distribution of the data. We can see the range for each variable, the mean and the quantiles.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;summary(housing)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
      CRIM                ZN             INDUS            CHAS              NOX               RM             AGE              DIS              RAD              TAX           PTRATIO     
 Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000   Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130   Min.   : 1.000   Min.   :187.0   Min.   :12.60  
 1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000   1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100   1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40  
 Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000   Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207   Median : 5.000   Median :330.0   Median :19.05  
 Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917   Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795   Mean   : 9.549   Mean   :408.2   Mean   :18.46  
 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000   3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188   3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20  
 Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000   Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127   Max.   :24.000   Max.   :711.0   Max.   :22.00  
       B              LSTAT            MEDV      
 Min.   :  0.32   Min.   : 1.73   Min.   : 5.00  
 1st Qu.:375.38   1st Qu.: 6.95   1st Qu.:17.02  
 Median :391.44   Median :11.36   Median :21.20  
 Mean   :356.67   Mean   :12.65   Mean   :22.53  
 3rd Qu.:396.23   3rd Qu.:16.95   3rd Qu.:25.00  
 Max.   :396.90   Max.   :37.97   Max.   :50.00&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;summary(price)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
5.00   17.02   21.20   22.53   25.00   50.00&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see that the minimum value of the variable “MEDV” (contained in the array “price”) is $5 and the maximum $50. That means that the range is $45. The mean and the median are quite close, $21.20 and $22.53 respectively.&lt;/p&gt;

&lt;h2 id=&quot;correlation-coefficients&quot;&gt;Correlation coefficients&lt;/h2&gt;
&lt;p&gt;Let’s find out the level of correlation of each variable.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;corr_matrix &amp;lt;- cor(housing); corr_matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
               CRIM          ZN       INDUS         CHAS         NOX          RM         AGE         DIS          RAD         TAX    PTRATIO           B      LSTAT       MEDV
CRIM     1.00000000 -0.20046922  0.40658341 -0.055891582  0.42097171 -0.21924670  0.35273425 -0.37967009  0.625505145  0.58276431  0.2899456 -0.38506394  0.4556215 -0.3883046
ZN      -0.20046922  1.00000000 -0.53382819 -0.042696719 -0.51660371  0.31199059 -0.56953734  0.66440822 -0.311947826 -0.31456332 -0.3916785  0.17552032 -0.4129946  0.3604453
INDUS    0.40658341 -0.53382819  1.00000000  0.062938027  0.76365145 -0.39167585  0.64477851 -0.70802699  0.595129275  0.72076018  0.3832476 -0.35697654  0.6037997 -0.4837252
CHAS    -0.05589158 -0.04269672  0.06293803  1.000000000  0.09120281  0.09125123  0.08651777 -0.09917578 -0.007368241 -0.03558652 -0.1215152  0.04878848 -0.0539293  0.1752602
NOX      0.42097171 -0.51660371  0.76365145  0.091202807  1.00000000 -0.30218819  0.73147010 -0.76923011  0.611440563  0.66802320  0.1889327 -0.38005064  0.5908789 -0.4273208
RM      -0.21924670  0.31199059 -0.39167585  0.091251225 -0.30218819  1.00000000 -0.24026493  0.20524621 -0.209846668 -0.29204783 -0.3555015  0.12806864 -0.6138083  0.6953599
AGE      0.35273425 -0.56953734  0.64477851  0.086517774  0.73147010 -0.24026493  1.00000000 -0.74788054  0.456022452  0.50645559  0.2615150 -0.27353398  0.6023385 -0.3769546
DIS     -0.37967009  0.66440822 -0.70802699 -0.099175780 -0.76923011  0.20524621 -0.74788054  1.00000000 -0.494587930 -0.53443158 -0.2324705  0.29151167 -0.4969958  0.2499287
RAD      0.62550515 -0.31194783  0.59512927 -0.007368241  0.61144056 -0.20984667  0.45602245 -0.49458793  1.000000000  0.91022819  0.4647412 -0.44441282  0.4886763 -0.3816262
TAX      0.58276431 -0.31456332  0.72076018 -0.035586518  0.66802320 -0.29204783  0.50645559 -0.53443158  0.910228189  1.00000000  0.4608530 -0.44180801  0.5439934 -0.4685359
PTRATIO  0.28994558 -0.39167855  0.38324756 -0.121515174  0.18893268 -0.35550149  0.26151501 -0.23247054  0.464741179  0.46085304  1.0000000 -0.17738330  0.3740443 -0.5077867
B       -0.38506394  0.17552032 -0.35697654  0.048788485 -0.38005064  0.12806864 -0.27353398  0.29151167 -0.444412816 -0.44180801 -0.1773833  1.00000000 -0.3660869  0.3334608
LSTAT    0.45562148 -0.41299457  0.60379972 -0.053929298  0.59087892 -0.61380827  0.60233853 -0.49699583  0.488676335  0.54399341  0.3740443 -0.36608690  1.0000000 -0.7376627
MEDV    -0.38830461  0.36044534 -0.48372516  0.175260177 -0.42732077  0.69535995 -0.37695457  0.24992873 -0.381626231 -0.46853593 -0.5077867  0.33346082 -0.7376627  1.0000000&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The best way to understand the correlations is visualizing them on a heatmap. For that, we need to load the &lt;code&gt;corrplot&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(corrplot)
corrplot.mixed(corr_matrix, upper=&quot;circle&quot;, number.cex=.8, tl.cex=.8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/images/2022-09-16/corr_plot.png&quot; alt=&quot;Housing correlation plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that the variables “RAD” and “TAX” are highly correlated, with a correlation coefficient of 0.91. We could consider droping one of them as a way to reduce dimensionality, but in this post we want to concentrate on PCA.&lt;/p&gt;

&lt;p&gt;Now we zoom in and check the correlation of each feature with the target. The variable with the lowest correlation with the price seems to be “NOX” and the one with the highest correlation “LSTAT”. In this case, the correlation coefficient is negative, -0.74, this means that as the % lower status of the population decreases, the median value of the homes increases.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;corr_array &amp;lt;- cor(features, price); corr_array
max_corr &amp;lt;- which.max(abs(corr_array))
min_corr &amp;lt;- which.min(abs(corr_array))
paste(&quot;The variable with the highest correlation is: &quot;, colnames(housing)[max_corr])
paste(&quot;The variable with the lowest correlation is: &quot;, colnames(housing)[min_corr])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
              [,1]
CRIM    -0.3883046
ZN       0.3604453
INDUS   -0.4837252
CHAS     0.1752602
NOX     -0.4273208
RM       0.6953599
AGE     -0.3769546
DIS      0.2499287
RAD     -0.3816262
TAX     -0.4685359
PTRATIO -0.5077867
B        0.3334608
LSTAT   -0.7376627
[1] &quot;The variable with the highest correlation is:  LSTAT&quot;
[1] &quot;The variable with the lowest correlation is:  CHAS&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can show as scatter plot of both of them in order to corrobrate this information visually. The first plot clearly shows that “CHAS” is a binary variable with a very low correlation with price.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plot(price, housing$CHAS, ylab=&quot;CHAS&quot;, main=&quot;Price vs. CHAS&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/images/2022-09-16/scatter_price_chas.png&quot; alt=&quot;Price vs. CHAS scatter plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this plot we do see a better inverse correlation between “LSTAT” and prices.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plot(price, housing$LSTAT, ylab=&quot;LSTAT&quot;, main=&quot;Price vs. LSTAT&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/images/2022-09-16/scatter_price_lstat.png&quot; alt=&quot;Price vs. CHAS scatter plot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pca-step-by-step&quot;&gt;PCA step by step&lt;/h2&gt;
&lt;p&gt;The variables in our dataset have different scales; for example, “NOX” goes from 0.39 to 0.87, while “TAX” goes from 187 to 711. If we directly apply PCA to the data as is, the features with larger scales will end up having more weight in our model. In order to avoid that, we need to sclale our data. There are different methods, here we will be performing a standardization, such that the resulting distribution will have a mean of 0 and a standard deviation of 1.&lt;/p&gt;

&lt;p&gt;We can either use the function &lt;code&gt;scale&lt;/code&gt; or the formula:
&lt;img src=&quot;/images/2022-09-16/scale_formula.svg&quot; alt=&quot;Scale formula&quot; /&gt;
First, we will try using the formula. We can do that with a for loop to calculate the mean and the standard deviation of each column and then apply the formula to each observation. Later, we calculate the mean and the standard deviation of each feature.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;temp_array &amp;lt;- c()
for (i in 1:13) {
  m &amp;lt;- mean(features[,i])
  s &amp;lt;- sd(features[,i])
  for (f in features[,i]) {
    d &amp;lt;- (f-m)/s
    temp_array &amp;lt;- append(temp_array, d)
  }
}
scaled_features2 &amp;lt;- matrix(temp_array, nrow=506, byrow=FALSE)

print(&quot;Array of means by column:&quot;)
round(apply(scaled_features2, 2, mean), 2)
print(&quot;Array of standard deviations by column:&quot;)
apply(scaled_features2, 2, sd)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
[1] &quot;Array of means by column:&quot;
 [1] 0 0 0 0 0 0 0 0 0 0 0 0 0
[1] &quot;Array of standard deviations by column:&quot;
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We obtained the results we expected, the means are 0 and all the standard deviations are 1. We have successfully standardized our data!&lt;/p&gt;

&lt;p&gt;Now we will see if we get the same results with the function &lt;code&gt;scale&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scaled_features &amp;lt;- scale(features, center = TRUE,scale = TRUE)

print(&quot;Array of means by column:&quot;)
round(apply(scaled_features, 2, mean), 2)
print(&quot;Array of standard deviations by column:&quot;)
apply(scaled_features, 2, sd)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
[1] &quot;Array of means by column:&quot;
   CRIM      ZN   INDUS    CHAS     NOX      RM     AGE     DIS     RAD     TAX PTRATIO       B   LSTAT 
      0       0       0       0       0       0       0       0       0       0       0       0       0 
[1] &quot;Array of standard deviations by column:&quot;
   CRIM      ZN   INDUS    CHAS     NOX      RM     AGE     DIS     RAD     TAX PTRATIO       B   LSTAT 
      1       1       1       1       1       1       1       1       1       1       1       1       1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once more, we obtain means equal to 0 and standard deviations equal to 1.&lt;/p&gt;

&lt;p&gt;Once we have our standardized values we need to calculate the covariance matrix. For that, we can, either use the formula or the function &lt;code&gt;cov&lt;/code&gt;. The formula to calculate the coraviance matrix is:
&lt;img src=&quot;/images/2022-09-16/covariance_formula.svg&quot; alt=&quot;Covariance formula&quot; /&gt;
Where &lt;em&gt;C_X&lt;/em&gt; is the coraviance matrix, &lt;em&gt;n&lt;/em&gt; is the total number of observations, &lt;em&gt;X&lt;/em&gt; is the matrix with our scaled data.&lt;/p&gt;

&lt;p&gt;We can apply the formula as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;t(scaled_features)%*%scaled_features/505
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
               CRIM          ZN       INDUS         CHAS         NOX          RM         AGE         DIS          RAD         TAX    PTRATIO           B      LSTAT
CRIM     1.00000000 -0.20046922  0.40658341 -0.055891582  0.42097171 -0.21924670  0.35273425 -0.37967009  0.625505145  0.58276431  0.2899456 -0.38506394  0.4556215
ZN      -0.20046922  1.00000000 -0.53382819 -0.042696719 -0.51660371  0.31199059 -0.56953734  0.66440822 -0.311947826 -0.31456332 -0.3916785  0.17552032 -0.4129946
INDUS    0.40658341 -0.53382819  1.00000000  0.062938027  0.76365145 -0.39167585  0.64477851 -0.70802699  0.595129275  0.72076018  0.3832476 -0.35697654  0.6037997
CHAS    -0.05589158 -0.04269672  0.06293803  1.000000000  0.09120281  0.09125123  0.08651777 -0.09917578 -0.007368241 -0.03558652 -0.1215152  0.04878848 -0.0539293
NOX      0.42097171 -0.51660371  0.76365145  0.091202807  1.00000000 -0.30218819  0.73147010 -0.76923011  0.611440563  0.66802320  0.1889327 -0.38005064  0.5908789
RM      -0.21924670  0.31199059 -0.39167585  0.091251225 -0.30218819  1.00000000 -0.24026493  0.20524621 -0.209846668 -0.29204783 -0.3555015  0.12806864 -0.6138083
AGE      0.35273425 -0.56953734  0.64477851  0.086517774  0.73147010 -0.24026493  1.00000000 -0.74788054  0.456022452  0.50645559  0.2615150 -0.27353398  0.6023385
DIS     -0.37967009  0.66440822 -0.70802699 -0.099175780 -0.76923011  0.20524621 -0.74788054  1.00000000 -0.494587930 -0.53443158 -0.2324705  0.29151167 -0.4969958
RAD      0.62550515 -0.31194783  0.59512927 -0.007368241  0.61144056 -0.20984667  0.45602245 -0.49458793  1.000000000  0.91022819  0.4647412 -0.44441282  0.4886763
TAX      0.58276431 -0.31456332  0.72076018 -0.035586518  0.66802320 -0.29204783  0.50645559 -0.53443158  0.910228189  1.00000000  0.4608530 -0.44180801  0.5439934
PTRATIO  0.28994558 -0.39167855  0.38324756 -0.121515174  0.18893268 -0.35550149  0.26151501 -0.23247054  0.464741179  0.46085304  1.0000000 -0.17738330  0.3740443
B       -0.38506394  0.17552032 -0.35697654  0.048788485 -0.38005064  0.12806864 -0.27353398  0.29151167 -0.444412816 -0.44180801 -0.1773833  1.00000000 -0.3660869
LSTAT    0.45562148 -0.41299457  0.60379972 -0.053929298  0.59087892 -0.61380827  0.60233853 -0.49699583  0.488676335  0.54399341  0.3740443 -0.36608690  1.0000000&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the function &lt;code&gt;cov&lt;/code&gt; we will get the same results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cov_matrix &amp;lt;- cov(scaled_features); cov_matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
               CRIM          ZN       INDUS         CHAS         NOX          RM         AGE         DIS          RAD         TAX    PTRATIO           B      LSTAT
CRIM     1.00000000 -0.20046922  0.40658341 -0.055891582  0.42097171 -0.21924670  0.35273425 -0.37967009  0.625505145  0.58276431  0.2899456 -0.38506394  0.4556215
ZN      -0.20046922  1.00000000 -0.53382819 -0.042696719 -0.51660371  0.31199059 -0.56953734  0.66440822 -0.311947826 -0.31456332 -0.3916785  0.17552032 -0.4129946
INDUS    0.40658341 -0.53382819  1.00000000  0.062938027  0.76365145 -0.39167585  0.64477851 -0.70802699  0.595129275  0.72076018  0.3832476 -0.35697654  0.6037997
CHAS    -0.05589158 -0.04269672  0.06293803  1.000000000  0.09120281  0.09125123  0.08651777 -0.09917578 -0.007368241 -0.03558652 -0.1215152  0.04878848 -0.0539293
NOX      0.42097171 -0.51660371  0.76365145  0.091202807  1.00000000 -0.30218819  0.73147010 -0.76923011  0.611440563  0.66802320  0.1889327 -0.38005064  0.5908789
RM      -0.21924670  0.31199059 -0.39167585  0.091251225 -0.30218819  1.00000000 -0.24026493  0.20524621 -0.209846668 -0.29204783 -0.3555015  0.12806864 -0.6138083
AGE      0.35273425 -0.56953734  0.64477851  0.086517774  0.73147010 -0.24026493  1.00000000 -0.74788054  0.456022452  0.50645559  0.2615150 -0.27353398  0.6023385
DIS     -0.37967009  0.66440822 -0.70802699 -0.099175780 -0.76923011  0.20524621 -0.74788054  1.00000000 -0.494587930 -0.53443158 -0.2324705  0.29151167 -0.4969958
RAD      0.62550515 -0.31194783  0.59512927 -0.007368241  0.61144056 -0.20984667  0.45602245 -0.49458793  1.000000000  0.91022819  0.4647412 -0.44441282  0.4886763
TAX      0.58276431 -0.31456332  0.72076018 -0.035586518  0.66802320 -0.29204783  0.50645559 -0.53443158  0.910228189  1.00000000  0.4608530 -0.44180801  0.5439934
PTRATIO  0.28994558 -0.39167855  0.38324756 -0.121515174  0.18893268 -0.35550149  0.26151501 -0.23247054  0.464741179  0.46085304  1.0000000 -0.17738330  0.3740443
B       -0.38506394  0.17552032 -0.35697654  0.048788485 -0.38005064  0.12806864 -0.27353398  0.29151167 -0.444412816 -0.44180801 -0.1773833  1.00000000 -0.3660869
LSTAT    0.45562148 -0.41299457  0.60379972 -0.053929298  0.59087892 -0.61380827  0.60233853 -0.49699583  0.488676335  0.54399341  0.3740443 -0.36608690  1.0000000&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This covariance matrix measures the level of lineal correlation of each pair of variables. Since our values are standardized, this matrix is the same as the correlation matrix we calculated earlier on. The values on the main diagonal correspond to the variance of each variable, in this case they are all 1.&lt;/p&gt;

&lt;p&gt;With the PCA we want to find a transformation matrix &lt;em&gt;P&lt;/em&gt; that multiplied with our strandardized matrix &lt;em&gt;X&lt;/em&gt; gives us a matrix &lt;em&gt;T&lt;/em&gt;, such that if we calculate the covariance matrix of &lt;em&gt;T&lt;/em&gt; we obtain a diagonal matrix. (The items outside the main diagonal are all 0.). &lt;em&gt;P&lt;/em&gt; is a matrix where the columns are the eigen vectors of the covariance matrix &lt;em&gt;C_X&lt;/em&gt; and the covariance matrix of the transformed data &lt;em&gt;T&lt;/em&gt; will have the eigen values on the main diagonal. These are usually organized from largest to smallest.&lt;/p&gt;

&lt;p&gt;Let’s calculate the matrix &lt;em&gt;P&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;P &amp;lt;- eigen(cov_matrix)$vectors; P
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
              [,1]        [,2]        [,3]        [,4]         [,5]        [,6]         [,7]         [,8]        [,9]        [,10]       [,11]        [,12]        [,13]
 [1,]  0.250951397  0.31525237 -0.24656649 -0.06177071  0.082156919 -0.21965961  0.777607207  0.153350477  0.26039028 -0.019369130 -0.10964435  0.086761070  0.045952304
 [2,] -0.256314541  0.32331290 -0.29585782 -0.12871159  0.320616987 -0.32338810 -0.274996280 -0.402680309  0.35813749 -0.267527234  0.26275629 -0.071425278 -0.080918973
 [3,]  0.346672065 -0.11249291  0.01594592 -0.01714571 -0.007811194 -0.07613790 -0.339576454  0.173931716  0.64441615  0.363532262 -0.30316943 -0.113199629 -0.251076540
 [4,]  0.005042434 -0.45482914 -0.28978082 -0.81594136  0.086530945  0.16749014  0.074136208 -0.024662148 -0.01372777  0.006181836  0.01392667 -0.003982683  0.035921715
 [5,]  0.342852313 -0.21911553 -0.12096411  0.12822614  0.136853557 -0.15298267 -0.199634840  0.080120560 -0.01852201 -0.231056455  0.11131888  0.804322567  0.043630446
 [6,] -0.189242570 -0.14933154 -0.59396117  0.28059184 -0.423447195  0.05926707  0.063939924 -0.326752259  0.04789804  0.431420193  0.05316154  0.152872864  0.045567096
 [7,]  0.313670596 -0.31197778  0.01767481  0.17520603  0.016690847 -0.07170914  0.116010713 -0.600822917 -0.06756218 -0.362778957 -0.45915939 -0.211936074 -0.038550683
 [8,] -0.321543866  0.34907000  0.04973627 -0.21543585  0.098592247  0.02343872 -0.103900440 -0.121811982 -0.15329124  0.171213138 -0.69569257  0.390941129 -0.018298538
 [9,]  0.319792768  0.27152094 -0.28725483 -0.13234996 -0.204131621 -0.14319401 -0.137942546  0.080358311 -0.47089067 -0.021909452  0.03654388 -0.107025890 -0.633489720
[10,]  0.338469147  0.23945365 -0.22074447 -0.10333509 -0.130460565 -0.19293428 -0.314886835  0.082774347 -0.17656339  0.035168348 -0.10483575 -0.215191126  0.720233448
[11,]  0.204942258  0.30589695  0.32344627 -0.28262198 -0.584002232  0.27315330  0.002323869 -0.317884202  0.25442836 -0.153430488  0.17450534  0.209598826  0.023398052
[12,] -0.202972612 -0.23855944  0.30014590 -0.16849850 -0.345606947 -0.80345454  0.070294759 -0.004922915 -0.04489802  0.096515117  0.01927490  0.041723158 -0.004463073
[13,]  0.309759840  0.07432203  0.26700025 -0.06941441  0.394561129 -0.05321583  0.087011169 -0.424352926 -0.19522139  0.600711409  0.27138243  0.055225960  0.024431677&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we can see the eigen values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;eigen(cov_matrix)$values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
[1] 6.12684883 1.43327512 1.24261667 0.85757511 0.83481594 0.65740718 0.53535609 0.39609731 0.27694333 0.22023782 0.18601437 0.16930298 0.06350926&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The eigen vector associated with the largest eigen value is usually called “first principal component”. We can calculate how much of the total data variability is the first principal component like this.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;eigen_v &amp;lt;- eigen(cov_matrix)$values
PC1v &amp;lt;- eigen_v[1]/sum(eigen_v)*100; PC1v
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
[1] 47.12961&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we wanted to keep, for example, 80% of the variance in our data, we would need to keep the first 5 principal components. This is already a significant reduction to the dimension of our original dataset, we have gone from 13 to 5 columns.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for(nPC in 1:13){
  if ((sum(eigen_v[1:nPC])/sum(eigen_v)*100) &amp;gt; 80){
    print(nPC)
    print(paste(&quot;Total variance retained:&quot;, sum(eigen_v[1:nPC])/sum(eigen_v)*100))
    break
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
[1] 5
[1] &quot;Total variance retained: 80.7317820504591&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From our eigen vectors we can also figure out which variables are more important to each of the principal components. We can see, for example, that the two most important variables in the first principal component are “INDUS” and “NOX”. And the two most important variables in the second principal component are “CHAS” and “DIS”.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;P[,1]
max_v &amp;lt;- which.max(abs(P[,1]))
print(paste(&quot;Most important variable in PC1:&quot;, colnames(housing[max_v])))
max_v2 &amp;lt;- which.max(abs(P[-max_v,1]))
print(paste(&quot;Second most important variable in PC1:&quot;, colnames(housing[-max_v][max_v2])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
 [1]  0.250951397 -0.256314541  0.346672065  0.005042434  0.342852313 -0.189242570  0.313670596 -0.321543866  0.319792768  0.338469147  0.204942258 -0.202972612  0.309759840
[1] &quot;Most important variable in PC1: INDUS&quot;
[1] &quot;Second most important variable in PC1: NOX&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;P[,2]
max_v &amp;lt;- which.max(abs(P[,2]))
print(paste(&quot;Most important variable in PC2:&quot;, colnames(housing[max_v])))
max_v2 &amp;lt;- which.max(abs(P[-max_v,2]))
print(paste(&quot;Second most important variable in PC2:&quot;, colnames(housing[-max_v][max_v2])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
 [1]  0.31525237  0.32331290 -0.11249291 -0.45482914 -0.21911553 -0.14933154 -0.31197778  0.34907000  0.27152094  0.23945365  0.30589695 -0.23855944  0.07432203
[1] &quot;Most important variable in PC2: CHAS&quot;
[1] &quot;Second most important variable in PC2: DIS&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the matrix &lt;em&gt;P&lt;/em&gt; we can calculate our transformed matrix &lt;em&gt;T&lt;/em&gt; and we can see that the covariance matrix of &lt;em&gt;T&lt;/em&gt; is a diagonal matrix, the diagonal of which contains the eigen values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;T &amp;lt;- scaled_features%*%P
round(cov(T), 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
 [1,] 6.13 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00  0.00  0.00  0.00  0.00
 [2,] 0.00 1.43 0.00 0.00 0.00 0.00 0.00  0.0 0.00  0.00  0.00  0.00  0.00
 [3,] 0.00 0.00 1.24 0.00 0.00 0.00 0.00  0.0 0.00  0.00  0.00  0.00  0.00
 [4,] 0.00 0.00 0.00 0.86 0.00 0.00 0.00  0.0 0.00  0.00  0.00  0.00  0.00
 [5,] 0.00 0.00 0.00 0.00 0.83 0.00 0.00  0.0 0.00  0.00  0.00  0.00  0.00
 [6,] 0.00 0.00 0.00 0.00 0.00 0.66 0.00  0.0 0.00  0.00  0.00  0.00  0.00
 [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.54  0.0 0.00  0.00  0.00  0.00  0.00
 [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.4 0.00  0.00  0.00  0.00  0.00
 [9,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.28  0.00  0.00  0.00  0.00
[10,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00  0.22  0.00  0.00  0.00
[11,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00  0.00  0.19  0.00  0.00
[12,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00  0.00  0.00  0.17  0.00
[13,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00  0.00  0.00  0.00  0.06&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;pca-using-prcomp&quot;&gt;PCA using prcomp()&lt;/h2&gt;
&lt;p&gt;There is an R built-in function that calculates the principal components for us, &lt;code&gt;prcomp&lt;/code&gt;. It essentially gives us the matrix &lt;em&gt;P&lt;/em&gt; that we calculated previously.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PCA &amp;lt;- prcomp(features, scale = TRUE); PCA
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
Standard deviations (1, .., p=13):
 [1] 2.4752472 1.1971947 1.1147272 0.9260535 0.9136826 0.8108065 0.7316803 0.6293626 0.5262541 0.4692950 0.4312938 0.4114644 0.2520104

Rotation (n x k) = (13 x 13):
                 PC1         PC2         PC3         PC4          PC5         PC6          PC7          PC8         PC9         PC10        PC11         PC12         PC13
CRIM     0.250951397 -0.31525237  0.24656649 -0.06177071  0.082156919 -0.21965961  0.777607207 -0.153350477  0.26039028 -0.019369130 -0.10964435 -0.086761070  0.045952304
ZN      -0.256314541 -0.32331290  0.29585782 -0.12871159  0.320616987 -0.32338810 -0.274996280  0.402680309  0.35813749 -0.267527234  0.26275629  0.071425278 -0.080918973
INDUS    0.346672065  0.11249291 -0.01594592 -0.01714571 -0.007811194 -0.07613790 -0.339576454 -0.173931716  0.64441615  0.363532262 -0.30316943  0.113199629 -0.251076540
CHAS     0.005042434  0.45482914  0.28978082 -0.81594136  0.086530945  0.16749014  0.074136208  0.024662148 -0.01372777  0.006181836  0.01392667  0.003982683  0.035921715
NOX      0.342852313  0.21911553  0.12096411  0.12822614  0.136853557 -0.15298267 -0.199634840 -0.080120560 -0.01852201 -0.231056455  0.11131888 -0.804322567  0.043630446
RM      -0.189242570  0.14933154  0.59396117  0.28059184 -0.423447195  0.05926707  0.063939924  0.326752259  0.04789804  0.431420193  0.05316154 -0.152872864  0.045567096
AGE      0.313670596  0.31197778 -0.01767481  0.17520603  0.016690847 -0.07170914  0.116010713  0.600822917 -0.06756218 -0.362778957 -0.45915939  0.211936074 -0.038550683
DIS     -0.321543866 -0.34907000 -0.04973627 -0.21543585  0.098592247  0.02343872 -0.103900440  0.121811982 -0.15329124  0.171213138 -0.69569257 -0.390941129 -0.018298538
RAD      0.319792768 -0.27152094  0.28725483 -0.13234996 -0.204131621 -0.14319401 -0.137942546 -0.080358311 -0.47089067 -0.021909452  0.03654388  0.107025890 -0.633489720
TAX      0.338469147 -0.23945365  0.22074447 -0.10333509 -0.130460565 -0.19293428 -0.314886835 -0.082774347 -0.17656339  0.035168348 -0.10483575  0.215191126  0.720233448
PTRATIO  0.204942258 -0.30589695 -0.32344627 -0.28262198 -0.584002232  0.27315330  0.002323869  0.317884202  0.25442836 -0.153430488  0.17450534 -0.209598826  0.023398052
B       -0.202972612  0.23855944 -0.30014590 -0.16849850 -0.345606947 -0.80345454  0.070294759  0.004922915 -0.04489802  0.096515117  0.01927490 -0.041723158 -0.004463073
LSTAT    0.309759840 -0.07432203 -0.26700025 -0.06941441  0.394561129 -0.05321583  0.087011169  0.424352926 -0.19522139  0.600711409  0.27138243 -0.055225960  0.024431677&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can obtain the matrix &lt;em&gt;T&lt;/em&gt; accessing &lt;code&gt;$x&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;head(PCA$x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
           PC1          PC2         PC3       PC4        PC5        PC6       PC7        PC8         PC9        PC10        PC11        PC12         PC13
[1,] -2.096223  0.772348426  0.34260368 0.8908924  0.4226521 -0.3150264 0.3183257 -0.2955393 -0.42451671 -0.63957348 -0.03296774 -0.01942101  0.365613514
[2,] -1.455811  0.591399952 -0.69451201 0.4869766 -0.1956820  0.2639620 0.5533137  0.2234488 -0.16679701 -0.08415319 -0.64017631  0.12567304 -0.070649578
[3,] -2.072547  0.599046578  0.16695638 0.7384734 -0.9336102  0.4476516 0.4840809 -0.1050622  0.06970615  0.18020170 -0.48707471 -0.13319472 -0.014007936
[4,] -2.608922 -0.006863826 -0.10018499 0.3433814 -1.1038636  0.6639924 0.6220253 -0.2556877 -0.34190738 -0.04585594 -0.35985263 -0.50817472  0.007839164
[5,] -2.455755  0.097615346 -0.07527372 0.4274838 -1.0648705  0.6164372 0.7043889  0.1343911 -0.41725486  0.14074052 -0.39475923 -0.49723967  0.014260153
[6,] -2.212662 -0.009477633 -0.67171636 0.1757362 -0.6265676  0.5708543 0.6505396 -0.1095817 -0.47347313 -0.37045163 -0.52705986 -0.30476248 -0.038886671&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the summary we can also see the cumulative proportion of the variance explained by each vector, as we found out before, if we take the first 5 princip al components, we will be retaining 80.73% of the information in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;summary(PCA)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
Importance of components:
                          PC1    PC2     PC3     PC4     PC5     PC6     PC7     PC8    PC9    PC10    PC11    PC12    PC13
Standard deviation     2.4752 1.1972 1.11473 0.92605 0.91368 0.81081 0.73168 0.62936 0.5263 0.46930 0.43129 0.41146 0.25201
Proportion of Variance 0.4713 0.1103 0.09559 0.06597 0.06422 0.05057 0.04118 0.03047 0.0213 0.01694 0.01431 0.01302 0.00489
Cumulative Proportion  0.4713 0.5816 0.67713 0.74310 0.80732 0.85789 0.89907 0.92954 0.9508 0.96778 0.98209 0.99511 1.00000&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;linear-regression-example&quot;&gt;Linear regression example&lt;/h2&gt;
&lt;p&gt;We can use the first 5 columns of our matrix &lt;em&gt;T&lt;/em&gt; to create a linear regression model that could be used to predict the housing prices. We won’t go into much detail, but it is an example of a use case for our transformed data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;T5 &amp;lt;- T[,1: 5]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to scale the target variable as well so that it matches the rest of our data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scaled_price &amp;lt;- scale(price, center = TRUE, scale = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use our transformed matrix to calculate a linear regression model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model &amp;lt;- lm(formula=scaled_price ~ T5)
summary(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
Call:
lm(formula = scaled_price ~ T5)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.1486 -0.3146 -0.0824  0.1878  3.5987 

Coefficients:
              Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept) -3.428e-16  2.459e-02   0.000        1    
T51         -2.471e-01  9.946e-03 -24.850  &amp;lt; 2e-16 ***
T52         -2.387e-01  2.056e-02 -11.606  &amp;lt; 2e-16 ***
T53         -3.807e-01  2.208e-02 -17.237  &amp;lt; 2e-16 ***
T54          1.175e-01  2.658e-02   4.420 1.21e-05 ***
T55         -2.428e-01  2.694e-02  -9.012  &amp;lt; 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5532 on 500 degrees of freedom
Multiple R-squared:  0.697,	Adjusted R-squared:  0.6939 
F-statistic:   230 on 5 and 500 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We obtain a coefficient of determination or R-squared of 0.697. That means our model explains about 70% of the variation in housing prices.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1-(var(residuals(model))/var(scaled_price))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;output&quot;&gt;&lt;code&gt;## OUTPUT
          [,1]
[1,] 0.6969794&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post we have learned how to analyze the correlations between variables. We have also performed a PCA analysis step by step and then we have seen how to do the same easily using &lt;code&gt;prcomp()&lt;/code&gt;. Finally, we have seen a simple example of a linear regression model obtained from the transformed data that we generated in our PCA.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Clearing images from Obsidian</title>
   <link href="http://localhost:4000/2022/08/31/screenshots_obsidian/"/>
   <updated>2022-08-31T00:00:00-07:00</updated>
   <id>http://localhost:4000/2022/08/31/screenshots_obsidian</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://obsidian.md/&quot;&gt;Obsidian&lt;/a&gt; is an open source app that helps you create a second brain based on a local folder of plain text markdown files.
I have been using this tool for a while and I love it. I use it to manage my daily and weekly tasks, to track habits, to take notes, to store cooking recipes and much more.&lt;/p&gt;

&lt;p&gt;Today I am going to share how I solved a small issue that I encountered when copying and pasting images to my vault. &lt;!--more--&gt;When you take a screen shot and paste the image to one of your notes, Obsidian adds the image file to your vault and creates an embedded link in your note. This is something I find myself doing very often.&lt;/p&gt;

&lt;p&gt;However, if you later delete the note, or delete the link from your note because you no longer need it, the image file remains in your vault. Overtime, this can create a lot of noise and use unnecessary space in your hard drive. You can manually check which images are not in use and delete them, but it’s time consuming. In order to solve this issue I wrote this simple script that checks if the images in your vault appear in your notes and if they don’t, if removes them. It also outputs a list of deleted images so that you can double check that the correct images were deleted.&lt;/p&gt;

&lt;p&gt;You can check and download the complete script &lt;a href=&quot;https://github.com/BlueDunes/obsidian_cleanup_images.git&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Please bear in mind that this code deletes the files permanently. I wrote it for my personal use and share it here in case someone finds it useful. It works for me, but it might not work for you.&lt;/p&gt;

&lt;p&gt;I advise you to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Try the code first commenting the section that deletes the images and check the list of images that the script intends to delete.&lt;/li&gt;
  &lt;li&gt;Have a backup copy of your vault before running the script.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, this script only deals with .png images and I wrote it for a macOS terminal. If you have some other kind of media file or if you are working on Linux or Windows, you may need to modify the script.&lt;/p&gt;

&lt;p&gt;Let’s check the key parts of the code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd PATH_TO_YOUR_VAULT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This goes to the directory where your vault is located.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;for pic in `find . -type f -name &quot;*.png&quot;`; do
    massaged_pic=`echo $pic | rev | cut -d '/' -f1 | rev`

    occurrence=`grep -R $massaged_pic&quot;]&quot; . | wc -l`
	
    if [ &quot;$occurrence&quot; -eq &quot;0&quot; ]; then
        rm $pic
        echo &quot;removed: &quot;$pic
        echo $pic &amp;gt;&amp;gt; $list_img
    fi

done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finds all the .png files and for each of them cuts the string with the path at the “/” and takes the last part, which is the file name. Later, it counts the occurrences of that image in the vault. &lt;code&gt;-R&lt;/code&gt; means it’s checking inside the current folder and in it’s subfolders. If the image doesn’t appear in the vault, it deletes it and prints the name on the terminal, as well as adding it to a file containing a list of deleted images.&lt;/p&gt;

&lt;p&gt;Before running the file you need to write the path to your vault in the variable “path_to_vault” or next to the &lt;code&gt;cd&lt;/code&gt; command. In order to run the file use on the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./cleanup_image_obs.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope this is useful for someone. And remember, run this code under your own responsibility! It does delete the files permanently!&lt;/p&gt;
</content>
 </entry>
 

</feed>
